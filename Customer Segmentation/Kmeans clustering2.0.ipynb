{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import time\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import os\n",
    "import itertools\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "pd.set_option('max_columns', 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PROCESSED_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptDir = os.getcwd()\n",
    "relPath = r\"../Customer Segmentation/\"\n",
    "sampleFilePath = os.path.join(scriptDir, relPath, 'CJ filtered.csv')\n",
    "relPathOutput = r\"../Customer Segmentation/output/\"\n",
    "outputFolderPath = os.path.join(scriptDir, relPathOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe dimensions: (8809, 7)\n"
     ]
    }
   ],
   "source": [
    "# Load AirPartner Data\n",
    "df = pd.read_csv(sampleFilePath, sep=',', header=0, quotechar='\"', encoding='latin1')\n",
    "print('Dataframe dimensions:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accountid</th>\n",
       "      <th>enqid</th>\n",
       "      <th>product</th>\n",
       "      <th>brokercountry</th>\n",
       "      <th>owner</th>\n",
       "      <th>qtq_firstflightdate</th>\n",
       "      <th>Grossprofit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>column type</th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>null values (nb)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>null values (%)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Accountid  enqid product brokercountry   owner  \\\n",
       "column type          int64  int64  object        object  object   \n",
       "null values (nb)         0      0       0             0       0   \n",
       "null values (%)          0      0       0             0       0   \n",
       "\n",
       "                 qtq_firstflightdate Grossprofit  \n",
       "column type                   object     float64  \n",
       "null values (nb)                   0           0  \n",
       "null values (%)                    0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab_info = pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})\n",
    "tab_info = tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
    "tab_info = tab_info.append(pd.DataFrame(df.isnull().sum()/df.shape[0]*100).T.\n",
    "                         rename(index={0:'null values (%)'}))\n",
    "display(tab_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['product'] != 'Tour Ops']\n",
    "df = df[df['Grossprofit'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "- Intermediate dataset groupbyed by flightid\n",
    "- Final dataset grouped by customers\n",
    "- K-means clustering\n",
    "- Interpreting the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRFMC principle\n",
    "- LRFMC stands for 'LOAD-TIME-FFP_DATE', 'DAYS_FROM_LAST_TO_END', 'FLIGHT_COUNT_NUMBER', 'SEG_KM_SUM', and 'AVG_COMMISSION'.\n",
    "- It is a customer segmentation technique that uses past purchase behaviour to divide customers into groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, norm, probplot, boxcox\n",
    "from scipy.stats import kstest\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Accountid', 'enqid', 'product', 'brokercountry', 'owner',\n",
       "       'qtq_firstflightdate', 'Grossprofit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtq_firstflightdate: 2016-02-01 00:00:00 -> 2020-05-15 23:00:00\n"
     ]
    }
   ],
   "source": [
    "df['qtq_firstflightdate'] = pd.to_datetime(df['qtq_firstflightdate'])\n",
    "\n",
    "print('qtq_firstflightdate:', df['qtq_firstflightdate'].min(), '->', df['qtq_firstflightdate'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column that tells the Recency of deal\n",
    "def reduction_data(data):\n",
    "    data = data[['qtq_firstflightdate', 'Total Distance (NMi)', 'gross', 'companyid', 'enqid']]\n",
    "    data['LOAD_TIME'] = pd.to_datetime('22-07-2019')\n",
    "    data['DATEDIFF_BD'] = (data['LOAD_TIME'] - pd.to_datetime(data['booked date'])).dt.days\n",
    "\n",
    "    L_agg = data.groupby('companyid')['DATEDIFF_BD'].agg({'L':'max'})\n",
    "    R_agg = data.groupby('companyid')['DATEDIFF_BD'].agg({'R':'min'})\n",
    "    F_agg = data.groupby('companyid')['booked date'].agg({'F': lambda x: x.size})\n",
    "    M_agg = data.groupby('companyid')['Total Distance (NMi)'].agg({'M': 'sum'})\n",
    "    C_agg = data.groupby('companyid')['gross'].agg({'C':'mean'})\n",
    "    dataTransformed = (L_agg).join(R_agg).join(F_agg).join(M_agg).join(C_agg)\n",
    "    return dataTransformed\n",
    "\n",
    "dataTransformed = reduction_data(df)\n",
    "# Transformed the timedelta64 to int 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality\n",
    "- Histogram - Kurtosis and skewness\n",
    "- Normal probablity plot - Data distribution should closely follow the diagonal that represents the normal distributionm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length of time the customer relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qqplot(data, measure):\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "    (mu, sigma) = norm.fit(data)\n",
    "    \n",
    "    fig1 = fig.add_subplot(121)\n",
    "    sns.distplot(data, fit=norm, color='black')\n",
    "    fig1.set_title(measure + 'Distribution (mu={:.2f} and sigma = {:.2f})'.format(mu, sigma), loc='center')\n",
    "    fig1.set_xlabel(measure)\n",
    "    fig1.set_ylabel('Frequency')\n",
    "    \n",
    "    fig2 = fig.add_subplot(122)\n",
    "    res = probplot(data, plot=fig2)\n",
    "    fig2.set_title(measure + 'Probability Plot (skewness): {:.6f} and kurtosis: {:.6f})'.format(data.skew(), data.kurt()), loc='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig('L.png')\n",
    "    plt.show()\n",
    "\n",
    "qqplot(dataTransformed.L, 'Customer lifetime value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the first graph above we can see that sales recency distribution is skewed, has a peak on the left and a long tail to the right. It deviates from normal distribution and is positively biased.\n",
    "\n",
    "- From the Probability Plot, we could see that sales recency also does not align with the diagonal red line which represent normal distribution. The form of its distribution confirm that is a skewed right.\n",
    "\n",
    "- With skewness negative of 0.055, we confirm the lack of symmetry and indicate that sales recency are skewed right, as we can see at the Sales Distribution plot, skewed left means that the left tail is long relative to the right tail. The skewness for a normal distribution is zero, and any symmetric data should have a skewness near zero. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n",
    "\n",
    "- Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers, and positive kurtosis indicates a heavy-tailed distribution and negative kurtosis indicates a light tailed distribution. So, with 1.11 of positive kurtosis sales recency are heavy-tailed and has some outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(dataTransformed.R, 'Recency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the first graph above we can see that sales frequency distribution is skewed, has a peak on the left and a long tail to the right. It deviates from normal distribution and is positively biased.\n",
    "\n",
    "- From the Probability Plot, we could see that sales frequency also does **not align with the diagonal and confirm that is a skewed right.\n",
    "\n",
    "- With skewness positive of 12.1, we confirm the high lack of symmetry and with 249 Kurtosis indicates that is a heavy-tailed distribution and has outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(dataTransformed.F, 'Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the first graph above we can see that sales amount distribution is skewed, has a peak on the left and a long tail to the right. It deviates from normal distribution and is positively biased.\n",
    "\n",
    "- From the Probability Plot, we could see that sales amount also does not align with the diagonal, special on the right.\n",
    "\n",
    "- With skewness positive of 19.3, we confirm the high lack of symmetry and with 478 Kurtosis indicates that is a too heavy-tailed distribution and has outliers, surely more than 10 very extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gross mileage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(dataTransformed.M, 'Miles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average gross from each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(dataTransformed.C, 'Average gross from each customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataTransformed[dataTransformed.R < 720])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit_pdf(data, data_inst, fig, units, title):\n",
    "    from powerlaw import plot_ccdf, Fit, pdf, plot_pdf\n",
    "    ax = fig.add_subplot(n_graphs,n_data,data_inst)\n",
    "    \n",
    "    fit = Fit(data, discrete=True, xmin=1, fit_Method=\"KS\")\n",
    "    fit.plot_pdf(ax=ax, color='r', label = 'Exponential Data')\n",
    "    fit.power_law.plot_pdf(ax=ax, linestyle=':', color='g', label='Power-law fit')\n",
    "    fit.exponential.plot_pdf(ax=ax, linestyle='--', color='b', label='Exponential fit')\n",
    "    fit.lognormal.plot_pdf(ax=ax, linestyle='--', color='m', label='Lognormal fit')\n",
    "\n",
    "    plt.xlabel(units)\n",
    "    plt.title(title, fontsize = 12, fontweight = 'bold')\n",
    "    plt.legend(loc='lower left', fancybox=True)\n",
    "\n",
    "n_data = 1\n",
    "n_graphs = 1\n",
    "f = plt.figure(figsize=(10, 10))\n",
    "\n",
    "unit = 'Customer lifetime value'\n",
    "title = 'PDF of Lifetime Distribution'\n",
    "plot_fit_pdf(dataTransformed.L, 1, f, unit, title)\n",
    "plt.ylabel(\"$p(X\\geq x)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 1\n",
    "n_graphs = 1\n",
    "f = plt.figure(figsize=(10, 10))\n",
    "\n",
    "unit2 = 'Recency'\n",
    "title2 = 'PDF of Recency Distribution'\n",
    "plot_fit_pdf(dataTransformed.R, 1, f, unit2, title2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 3\n",
    "n_graphs = 1\n",
    "f = plt.figure(figsize=(16, 6))\n",
    "\n",
    "unit = 'Frequency'\n",
    "title = 'PDF of Frequency Distribution'\n",
    "plot_fit_pdf(dataTransformed.F, 1, f, unit, title)\n",
    "plt.ylabel(\"$p(X\\geq x)$\")\n",
    "\n",
    "unit = 'Monetary'\n",
    "title = 'PDF of Monetary Distribution'\n",
    "plot_fit_pdf(dataTransformed.M, 2, f, unit, title)\n",
    "plt.ylabel(\"$p(X\\geq x)$\")\n",
    "\n",
    "unit = 'Gross'\n",
    "title = 'PDF of Gross Distribution'\n",
    "plot_fit_pdf(dataTransformed.C, 3, f, unit, title)\n",
    "plt.ylabel(\"$p(X\\geq x)$\")\n",
    "f.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=.3, hspace=.2)\n",
    "f.savefig('PDF.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "dataTransformed_summary = dataTransformed.describe(percentiles = [], include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kstest(dataTransformed.iloc[:,0], 'norm'))\n",
    "print(kstest(dataTransformed.iloc[:,1], 'norm'))\n",
    "print(kstest(dataTransformed.iloc[:,2], 'norm'))\n",
    "print(kstest(dataTransformed.iloc[:,3], 'norm'))\n",
    "print(kstest(dataTransformed.iloc[:,4], 'norm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-transform to each vector\n",
    "- A problem is the huge range of values each variable can take, particularly noticeable for the monetary amount variable.\n",
    "- The log-transforamtion, along with the standardization, will ensure that the input to our algorithm is a homogenous set of scaled and transformed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTransformed['L_log'] = np.log10(dataTransformed['L'])\n",
    "dataTransformed['R_log'] = np.log10(dataTransformed['R'])\n",
    "dataTransformed['F_log'] = np.log10(dataTransformed['F'])\n",
    "dataTransformed['M_log'] = np.log10(dataTransformed['M'])\n",
    "dataTransformed['C_log'] = np.log10(dataTransformed['C'])\n",
    "\n",
    "feature_vector = ['L_log', 'R_log', 'F_log', 'M_log', 'C_log']\n",
    "subset = dataTransformed[feature_vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "- One of the requirements for K-means is the mean centering of the variable values.\n",
    "- Mean centering of a variable value means that we will replace the actual value of the variable with a standardized value, so that the variable has a mean of 0 and variance of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = subset.fillna(method='pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for i in subset.iloc[:, 1]:\n",
    "    if not np.isfinite(i):\n",
    "        print(index, i)\n",
    "    index += 1\n",
    "\n",
    "subset.iloc[324, 1] = subset.iloc[:, 1].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(data=subset[[\"L_log\",\"R_log\",\"F_log\", \"M_log\", \"C_log\"]],\n",
    "                    orient=\"h\", palette=\"mako\")\n",
    "plt.xlabel(\"Log10 Normalized Raw Value\")\n",
    "plt.title(\"LRFMC Outlier Identification\")\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization of RFM Features\n",
    "- Quintile Normalization:  a method used to segment a features’ distributions into ﬁve ordinal groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# subset['Scaled_L'] = scaler.fit_transform(subset[['L_log']])\n",
    "# subset['Scaled_R'] = scaler.fit_transform(subset[['R_log']])\n",
    "# subset['Scaled_F'] = scaler.fit_transform(subset[['F_log']])\n",
    "# subset['Scaled_M'] = scaler.fit_transform(subset[['M_log']])\n",
    "# subset['Scaled_C'] = scaler.fit_transform(subset[['C_log']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quartile = subset.quantile(q=[0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTransformed.iloc[:, :5].quantile(q=[0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_class(x, p, data):\n",
    "    if x <= data[p][0.25]:\n",
    "        return 4\n",
    "    elif x <= data[p][0.50]:\n",
    "        return 3\n",
    "    elif x <= data[p][0.75]: \n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def other_class(x, p, data):\n",
    "    if x <= data[p][0.25]:\n",
    "        return 1\n",
    "    elif x <= data[p][0.50]:\n",
    "        return 2\n",
    "    elif x <= data[p][0.75]: \n",
    "        return 3\n",
    "    else:\n",
    "        return 4   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfmc_seg = subset\n",
    "\n",
    "lrfmc_seg['L'] = lrfmc_seg['L_log'].apply(other_class, args=('L_log', Quartile))\n",
    "lrfmc_seg['R'] = lrfmc_seg['R_log'].apply(R_class, args=('R_log', Quartile))\n",
    "lrfmc_seg['F'] = lrfmc_seg['F_log'].apply(other_class, args=('F_log', Quartile))\n",
    "lrfmc_seg['M'] = lrfmc_seg['M_log'].apply(other_class, args=('M_log', Quartile))\n",
    "lrfmc_seg['C'] = lrfmc_seg['C_log'].apply(other_class, args=('C_log', Quartile))\n",
    "\n",
    "# combine the scores to create a single score.\n",
    "lrfmc_seg['LRFMC Class'] = lrfmc_seg.L.map(str) + lrfmc_seg.R.map(str) \\\n",
    "                        + lrfmc_seg.F.map(str) + lrfmc_seg.M.map(str) + lrfmc_seg.C.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfmc_seg.sort_values(by=['LRFMC Class', 'C_log'], ascending=[True, False]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lrfmc_seg[(lrfmc_seg['C'] == 4) & (lrfmc_seg['R'] == 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfmc_seg['Total Score'] = lrfmc_seg['L'] + lrfmc_seg['R'] \\\n",
    "                        + lrfmc_seg['F'] + lrfmc_seg['M'] + lrfmc_seg['C']\n",
    "\n",
    "lrfmc_table = lrfmc_seg[['L', 'R', 'F', 'M', 'C']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRFMC_Model = dataTransformed.iloc[:, :5]\n",
    "LRFMC_Model.columns = ['Lifetime', 'Recency', 'Frequency', 'SUM_Miles', 'Average GP']\n",
    "\n",
    "lrfmc_seg = pd.merge(lrfmc_seg, LRFMC_Model, on='companyid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRFMC_Model.quantile(q=[0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrfmc_seg.groupby('R')['Lifetime', 'Recency', 'Frequency', 'SUM_Miles', 'Average GP'].agg([np.mean, np.median, np.std])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction\n",
    "- K-means\n",
    "- silhouette score\n",
    "- Rader Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, calinski_harabaz_score\n",
    "from gap_statistic import OptimalK\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Elbow Method (WCSS)\n",
    "- Elbow method to check the best fit K between 2-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = lrfmc_table\n",
    "distorsions = []\n",
    "for k in range(1, 9):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X)\n",
    "    distorsions.append(kmeans.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(7,6), dpi=72)\n",
    "plt.plot(range(1, 9), distorsions, '-o', linewidth=0.9,  color=\"k\", markerfacecolor='w')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Total Within Sum of Square')\n",
    "plt.title('Normal/Quartile WCSS', fontweight='bold')\n",
    "plt.tick_params(direction='out',width=2,length=4)\n",
    "plt.axvline(x=3, color='black', linestyle = '--', alpha=0.7, linewidth=1)\n",
    "plt.axvline(x=5, color='black', linestyle = '--', alpha=0.7, linewidth=1)\n",
    "# fig.savefig('ElbowMethod.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette analysis on K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in range(2, 11):\n",
    "    labels = KMeans(n_clusters=k).fit(lrfmc_table).labels_\n",
    "    score = silhouette_score(X, labels)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,6), dpi=72)\n",
    "plt.plot(range(2, 11), scores, '-o', linewidth=0.9)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average silhouette width')\n",
    "plt.title('The Silhouette Method showing the optimal k', fontsize='large')\n",
    "plt.axvline(x=5, color='black', linestyle = '--', alpha=0.7, linewidth=1)\n",
    "plt.axvline(x=8, color='black', linestyle = '--', alpha=0.7, linewidth=1)\n",
    "# fig.savefig('SilhouetteMethod.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gap Statisitic\n",
    "- Construct the OptimalK class using the joblib backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimalK = OptimalK(parallel_backend='rust')\n",
    "optimalK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimalK(data, nrefs=3, maxClusters=15):\n",
    "    gaps = np.zeros((len(range(1, maxClusters)),))\n",
    "    resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})\n",
    "    for gap_index, k in enumerate(range(1, maxClusters)):\n",
    "\n",
    "        refDisps = np.zeros(nrefs)\n",
    "        for i in range(nrefs):\n",
    "                        \n",
    "            km = KMeans(k)\n",
    "            km.fit(lrfmc_table)\n",
    "            \n",
    "            refDisp = km.inertia_\n",
    "            refDisps[i] = refDisp\n",
    "\n",
    "        km = KMeans(k)\n",
    "        km.fit(data)\n",
    "        \n",
    "        origDisp = km.inertia_\n",
    "        gap = np.log(np.mean(refDisps)) - np.log(origDisp)\n",
    "        gaps[gap_index] = gap\n",
    "        \n",
    "        resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)\n",
    "\n",
    "    return (gaps.argmax() + 1, resultsdf)  # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, gapdf = optimalK(lrfmc_table, nrefs=5, maxClusters=11)\n",
    "print ('Optimal k is: ', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,6), dpi=72)\n",
    "plt.plot(gapdf.clusterCount, gapdf.gap, '-o', linewidth=0.9)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Gap statistic (k)')\n",
    "plt.title('The Gap Method showing the optimal k', fontsize='large')\n",
    "plt.axvline(x=5, color='black', linestyle = '--', alpha=0.7, linewidth=1)\n",
    "plt.axvline(x=8, color='black', linestyle = '--', alpha=0.7, linewidth=1)\n",
    "fig.savefig('output/GapMethod.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto Distribution of Cluster Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y = dataTransformed.iloc[:, :5]\n",
    "distorsions = []\n",
    "for k in range(1, 9):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(Y)\n",
    "    distorsions.append(kmeans.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(7,6), dpi=72)\n",
    "plt.plot(range(1, 9), distorsions, '-o', linewidth=0.9,  color=\"k\", markerfacecolor='w')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Total Within Sum of Square')\n",
    "plt.title('Pareto/Raw WCSS', fontweight='bold')\n",
    "# fig.savefig('ElbowMethod.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y1 = subset.iloc[:,:5]\n",
    "distorsions = []\n",
    "for k in range(1, 9):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(Y1)\n",
    "    distorsions.append(kmeans.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(7,6), dpi=72)\n",
    "plt.plot(range(1, 9), distorsions, '-o', linewidth=0.9,  color=\"k\", markerfacecolor='w')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Total Within Sum of Square')\n",
    "plt.title('Normal/Raw WCSS', fontweight='bold')\n",
    "plt.xticks(np.linspace(1, 9, 9))\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "# fig.savefig('ElbowMethod.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pareto_table = dataTransformed.iloc[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pareto_Quartile = Pareto_table.quantile([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pareto_table['Lifetime'] = Pareto_table['L'].apply(other_class, args=('L', Pareto_Quartile))\n",
    "Pareto_table['Recency'] = Pareto_table['R'].apply(R_class, args=('R', Pareto_Quartile))\n",
    "Pareto_table['Frequency'] = Pareto_table['F'].apply(other_class, args=('F', Pareto_Quartile))\n",
    "Pareto_table['Miles'] = Pareto_table['M'].apply(other_class, args=('M', Pareto_Quartile))\n",
    "Pareto_table['GrossProfit'] = Pareto_table['C'].apply(other_class, args=('C', Pareto_Quartile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2 = Pareto_table.iloc[:,-5:]\n",
    "distorsions = []\n",
    "for k in range(1, 9):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(Y2)\n",
    "    distorsions.append(kmeans.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(7,6), dpi=72)\n",
    "plt.plot(range(1, 9), distorsions, '-o', linewidth=0.9,  color=\"k\", markerfacecolor='w')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Total Within Sum of Square')\n",
    "plt.title('Pareto/Quartile WCSS', fontweight='bold')\n",
    "# fig.savefig('ElbowMethod.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster center \n",
    "- look at the cluster center values after returning them to normal values from the log and scaled version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodel = KMeans(n_clusters=5, init='k-means++', \n",
    "                n_init=10, max_iter=500, tol=1e-04,\n",
    "                random_state=101)\n",
    "kmodel_label = kmodel.fit_predict(lrfmc_table)\n",
    "\n",
    "# Make clustering analysing by K-means alg\n",
    "r1 = pd.Series(kmodel.labels_).value_counts() \n",
    "r2 = pd.DataFrame(kmodel.cluster_centers_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.columns = ['L', 'R', 'F', 'M', 'C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = np.arange(len(r2.columns))\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, sharey=True)\n",
    "a1 = sns.barplot(r2.columns, list(r2.iloc[0]), ax=axes[0], palette=('husl'))\n",
    "a1.set_title(1)\n",
    "for x, y in zip(y_pos, list(r2.iloc[0])):\n",
    "    a1.text(x+0.001, y-0.03, int(y+0.5), ha='center', va='bottom')\n",
    "\n",
    "a2 = sns.barplot(r2.columns, list(r2.iloc[1]), ax=axes[1], palette='husl')\n",
    "a2.set_title(2)\n",
    "for x, y in zip(y_pos, list(r2.iloc[1])):\n",
    "    a2.text(x+0.001, y-0.03, int(y+0.5), ha='center', va='bottom')\n",
    "\n",
    "a3 = sns.barplot(r2.columns, list(r2.iloc[2]), ax=axes[2], palette='husl')\n",
    "a3.set_title(3)\n",
    "for x, y in zip(y_pos, list(r2.iloc[2])):\n",
    "    a3.text(x+0.001, y-0.03, int(y+0.5), ha='center', va='bottom')\n",
    "\n",
    "a4 = sns.barplot(r2.columns, list(r2.iloc[3]), ax=axes[3], palette='husl')\n",
    "a4.set_title(4)\n",
    "for x, y in zip(y_pos, list(r2.iloc[3])):\n",
    "    a4.text(x+0.001, y-0.03, int(y+0.5), ha='center', va='bottom')\n",
    "\n",
    "a5 = sns.barplot(r2.columns, list(r2.iloc[4]), ax=axes[4], palette='husl')\n",
    "a5.set_title(5)\n",
    "for x, y in zip(y_pos, list(r2.iloc[4])):\n",
    "    a5.text(x+0.001, y-0.03, int(y+0.5), ha='center', va='bottom')\n",
    "\n",
    "fig.text(0.26, 0.95, 'Cluster Centroid LFRMC Characteristics')\n",
    "fig.text(0.04, 0.5, 'Normalized Feature Value', va='center', rotation='vertical')\n",
    "# fig.savefig('output/Cluster Centroid.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodel2 = KMeans(n_clusters=2, init='k-means++', \n",
    "                n_init=10,max_iter=500, tol=1e-04, \n",
    "                random_state=101)\n",
    "kmodel_label2 = kmodel2.fit_predict(lrfmc_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r4 = pd.Series(kmodel2.labels_).value_counts() \n",
    "r3 = pd.DataFrame(kmodel2.cluster_centers_) \n",
    "r3.columns = ['L', 'R', 'F', 'M', 'C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = np.arange(len(r3.columns))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharey=True)\n",
    "a1 = sns.barplot(r3.columns, list(r3.iloc[0]), ax=axes[1], palette=('husl'))\n",
    "a1.set_title(1)\n",
    "for x, y in zip(y_pos, list(r3.iloc[0])):\n",
    "    a1.text(x+0.001, y-0.03, '%.1f'%y, ha='center', va='bottom')\n",
    "\n",
    "a2 = sns.barplot(r3.columns, list(r3.iloc[1]), ax=axes[0], palette='husl')\n",
    "a2.set_title(2)\n",
    "for x, y in zip(y_pos, list(r3.iloc[1])):\n",
    "    a2.text(x+0.001, y-0.03, '%.1f'%y, ha='center', va='bottom')\n",
    "\n",
    "fig.text(0.26, 0.95, 'Cluster Centroid LFRMC Characteristics')\n",
    "fig.text(0.04, 0.5, 'Normalized Feature Value', va='center', rotation='vertical')\n",
    "fig.savefig('output/Cluster Centroid2.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the customer to a specific class\n",
    "cluster_result = pd.concat([lrfmc_table, pd.Series(kmodel.labels_, \n",
    "                index=lrfmc_table.index)], axis=1) #详细输出每个样本对应的类别\n",
    "cluster_result.columns = list(lrfmc_table.columns) + [u'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.concat([r2, r1], axis = 1)\n",
    "r.columns = list(lrfmc_table) + [u'Size'] \n",
    "r = r.reset_index(drop=False)\n",
    "r = r.rename(columns={'index':'Clus'})\n",
    "r = r.set_index('Clus', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r['Perc'] = round(r.iloc[:,[5]].div(r.iloc[:,[5]].sum(axis=0), axis=1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r['Lifetime value'] = np.power(10, r['L']).astype(int)\n",
    "# r['Last day'] = -np.power(10, (4-r['R'])).astype(int)\n",
    "# r['Day count'] = np.power(10, r['F']).astype(int)\n",
    "# r['Sum miles'] = np.power(10, r['M']).astype(int)\n",
    "# r['Gross Profit'] = np.power(10, r['C']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, init='k-means++', \n",
    "                n_init=10,max_iter=500, tol=1e-04, \n",
    "                random_state=101).fit(lrfmc_table)\n",
    "kmeans_pred = kmeans.predict(lrfmc_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radar map\n",
    "- Visualize the clustering results by the radar map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = r2.values.max()\n",
    "min = r2.values.min()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "center_num = r.iloc[:, :6].values\n",
    "feature = [\"Lifetime\", \"Recency\", \"Frequency\", \"Total Miles\", \"Average Gross Profit\"]\n",
    "N = len(feature)\n",
    "\n",
    "for i, v in enumerate(center_num):\n",
    "    angles = np.linspace(0, 2 * np.pi, N, endpoint=False)\n",
    "    center = np.concatenate((v[:-1], [v[0]]))\n",
    "    angles = np.concatenate((angles, [angles[0]]))\n",
    "    \n",
    "    ax.plot(angles, center, 'o-', linewidth=2, label=\"Nb.%dClusters, %d Customers\" % (i + 1, v[-1]))\n",
    "    ax.fill(angles, center, alpha=0.25)\n",
    "    ax.set_thetagrids(angles * 180 / np.pi, feature, fontsize=15)\n",
    "    ax.set_ylim(min - 0.35, max + 0.35)\n",
    "    plt.title('Cluster Segmentation Rader Map', fontsize=20)\n",
    "    ax.grid(True)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.45, 1.0), ncol=1, fancybox=True, shadow=True)\n",
    "\n",
    "# fig.savefig('RaderMap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge two dataset to a new dataset for the results explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_Bookings = pd.read_csv('Cj filtered.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cluster_result = cluster_result.reset_index(drop=False)\n",
    "\n",
    "company_Class = index_cluster_result[['companyid', 'Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dateset2 = pd.merge(CL_Bookings, company_Class, on='companyid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTransformed = dataTransformed.reset_index('companyid')\n",
    "companyd_class = index_cluster_result.loc[:,['companyid', 'Class']]\n",
    "dataTransformed = pd.merge(companyd_class, dataTransformed, on='companyid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = dataTransformed[dataTransformed['Class'] == 0]\n",
    "cluster_1 = dataTransformed[dataTransformed['Class'] == 1]\n",
    "cluster_2 = dataTransformed[dataTransformed['Class'] == 2]\n",
    "cluster_3 = dataTransformed[dataTransformed['Class'] == 3]\n",
    "cluster_4 = dataTransformed[dataTransformed['Class'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(10, 10), dpi=80)\n",
    "sns.distplot(cluster_0[\"L\"] , color=\"royalblue\", hist=False, ax=axes[0, 0], label='Class_0')\n",
    "sns.distplot(cluster_1[\"L\"] , color='green', hist=False,ax=axes[0, 0], label='Class=1')\n",
    "sns.distplot(cluster_2[\"L\"] , color='olive', hist=False,ax=axes[0, 0], label='Class=2')\n",
    "sns.distplot(cluster_3[\"L\"] , color='firebrick', hist=False,ax=axes[0, 0], label='Class=3')\n",
    "sns.distplot(cluster_4[\"L\"] , color='teal', hist=False,ax=axes[0, 0], label='Class=4')\n",
    "\n",
    "sns.distplot(cluster_0[\"R\"] , color=\"royalblue\", hist=False, ax=axes[0, 1], label='Class_0')\n",
    "sns.distplot(cluster_1[\"R\"] , color='green', hist=False,ax=axes[0, 1], label='Class=1')\n",
    "sns.distplot(cluster_2[\"R\"] , color='olive', hist=False,ax=axes[0, 1], label='Class=2')\n",
    "sns.distplot(cluster_3[\"R\"] , color='firebrick', hist=False,ax=axes[0, 1], label='Class=3')\n",
    "sns.distplot(cluster_4[\"R\"] , color='teal', hist=False,ax=axes[0, 1], label='Class=4')\n",
    "\n",
    "sns.distplot(cluster_0[\"F\"] , color=\"royalblue\", hist=False, ax=axes[1, 0], label='Class_0')\n",
    "sns.distplot(cluster_1[\"F\"] , color=\"green\", hist=False, ax=axes[1, 0], label='Class_1')\n",
    "sns.distplot(cluster_2[\"F\"] , color=\"olive\", hist=False, ax=axes[1, 0], label='Class_2')\n",
    "sns.distplot(cluster_3[\"F\"] , color=\"firebrick\", hist=False, ax=axes[1, 0], label='Class_3')\n",
    "sns.distplot(cluster_4[\"F\"] , color=\"teal\", hist=False, ax=axes[1, 0], label='Class_4')\n",
    "\n",
    "sns.distplot(cluster_0[\"M\"] , color=\"teal\", ax=axes[1, 1])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class=1(High net Industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = whole_dateset2.groupby(['Industry'], as_index=False)['Class'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_nb (dataset, i):\n",
    "    class_nb =whole_dateset2[(whole_dateset2['Class'] == i)][['Class', 'Industry']]\n",
    "    class_nb = class_nb.groupby(['Industry'], as_index=False)['Class'].count()\n",
    "    class_nb = class_nb.rename(columns = {'Class':'Count'})\n",
    "    class_nb = pd.merge(class_nb, whole, on='Industry', how='left')\n",
    "    class_nb['Perc'] = np.round(class_nb['Count'] / class_nb['Class'], 2)\n",
    "    class_nb = class_nb.sort_values(by='Perc', ascending=False)\n",
    "    return class_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1 = class_nb(whole_dateset2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9), dpi=80)\n",
    "g = sns.barplot(x=\"Perc\", y=\"Industry\", data=class_1, palette='Blues_d')\n",
    "g.set_title(\"Class = 1 (High Net) Industry Distribution\", fontweight='bold')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class=2 (General)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_2 = class_nb(whole_dateset2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 10), dpi=80)\n",
    "g = sns.barplot(x=\"Perc\", y=\"Industry\", data=class_2, palette='Blues_d')\n",
    "g.set_title(\"Class = 2 (General) Industry Distribution\", fontweight='bold')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class=3 (Retention) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_3 = class_nb(whole_dateset2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 10), dpi=80)\n",
    "g = sns.barplot(x=\"Perc\", y=\"Industry\", data=class_3, palette='Blues_d')\n",
    "g.set_title(\"Class = 3 (Retention) Industry Distribution\", fontweight='bold')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class=4 (High Potential) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_4 = class_nb(whole_dateset2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,10), dpi=80)\n",
    "g = sns.barplot(x=\"Perc\", y=\"Industry\", data=class_4, palette='Blues_d')\n",
    "g.set_title(\"Class = 4 (High Potential) Industry Distribution\", fontweight='bold')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class=5 (Low Net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_5 = class_nb(whole_dateset2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9), dpi=80)\n",
    "g = sns.barplot(x=\"Perc\", y=\"Industry\", data=class_5, palette='Blues_d')\n",
    "g.set_title(\"Class = 5 (Low Net) Industry Distribution\", fontweight='bold')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore results from the companyid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_num(dataset, i):\n",
    "    company_nb = dataset[(dataset['Class'] == i)][['Class', 'companyid']]\n",
    "    company_nb = company_nb.groupby(['companyid'], as_index=False)['Class'].count()\n",
    "    company_nb = company_nb.rename(columns = {'Class':'Count'})\n",
    "    company_nb['Perc'] = company_nb['Count'] / np.sum(company_nb['Count'], axis=0)\n",
    "    company_nb_head = company_nb.sort_values(by='Count', ascending=False)[:10]\n",
    "    \n",
    "    y_pos = np.array(company_nb_head['companyid'])\n",
    "    performance = np.array(company_nb_head['Count'])\n",
    "    \n",
    "    cols = []\n",
    "    for i in range(len(company_nb_head)):\n",
    "        col = 'company_{}'.format(y_pos[i])\n",
    "        cols.append(col)\n",
    "        \n",
    "    return company_nb_head, y_pos, performance, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_1_class, y_pos, performance, cols = company_num(whole_dateset2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "g = sns.barplot(performance,cols, palette='GnBu_d')\n",
    "g.set_title(\"Class = 1 (High New) Companyd Distribution\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_2_c, y_pos, performance, cols = company_num(whole_dateset2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "g = sns.barplot(performance, cols, palette='GnBu_d')\n",
    "g.set_title(\"Class = 2 (Retention) Companyd Distribution\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_3_class, y_pos, performance, cols = company_num(whole_dateset2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "g = sns.barplot(performance, cols, palette='GnBu_d')\n",
    "g.set_title(\"Class = 3 (Retention) Companyd Distribution\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_4_class, y_pos, performance, cols = company_num(whole_dateset2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "g = sns.barplot(performance, cols, palette='GnBu_d')\n",
    "g.set_title(\"Class = 4 (High Potential) Companyd Distribution\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_5_class, y_pos, performance, cols = company_num(whole_dateset2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "g = sns.barplot(performance, cols, palette='GnBu_d')\n",
    "g.set_title(\"Class = 5 (Low net) Companyd Distribution\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge two dataset to a new dataset for the final modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_Model = pd.read_csv('CL dataset.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dateset1 = pd.merge(CL_Model, company_Class, on='companyid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dateset1.to_csv('whole_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = cluster_result.pop(\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut = dict(zip(Class.unique(), \"rbg\"))\n",
    "row_colors = Class.map(lut)\n",
    "\n",
    "#First create the clustermap figure\n",
    "g = sns.clustermap(cluster_result, row_colors = row_colors, figsize=(13,8))\n",
    "# set the gridspec to only cover half of the figure\n",
    "g.gs.update(left=0.05, right=0.45)\n",
    "\n",
    "#create new gridspec for the right part\n",
    "gs2 = matplotlib.gridspec.GridSpec(1,1, left=0.6)\n",
    "# create axes within this new gridspec\n",
    "ax2 = g.fig.add_subplot(gs2[0])\n",
    "# plot boxplot in the new axes\n",
    "sns.boxplot(data=cluster_result, orient=\"h\", palette=\"Set2\", ax = ax2)\n",
    "fig.savefig('clustermap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA testing for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dateset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dateset1 = whole_dateset1.drop(['companyid'], axis=1)\n",
    "\n",
    "cols = [i for i in whole_dateset1.columns if i not in ['ProfitMargin(%)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(endog = whole_dateset1['ProfitMargin(%)'].astype(int), exog = whole_dateset1[cols]).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dateset1 = whole_dateset1.rename(columns = {'Product_Tour Ops': 'Product_Tour', 'ProfitMargin(%)':'ProfitMargin', 'Country_United Kingdom':'Country_UK', 'Country_United States':'Country_US'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'ProfitMargin ~ HashEncode_0 + HashEncode_1 + HashEncode_2 + HashEncode_3 + HashEncode_4 + HashEncode_5 + HashEncode_6 + HashEncode_7 + \\\n",
    "           Round_trip + Scaled_gross_log + Scaled_Distance_log + Scaled_FlightTime_log + Country_France + Country_Germany + Country_Italy + Country_Monaco + Country_Other + Country_Sweden + \\\n",
    "           Country_Switzerland + Country_Turkey + Country_UK + Country_US + Product_Tour + passengersRange_1 + passengersRange_2 + \\\n",
    "           passengersRange_3 + categ_aircraft + categ_0 + categ_1 + categ_2 + categ_3 + categ_4 + categ_5 + Class'\n",
    "results = ols(formula, data = whole_dateset1).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aov_table = sm.stats.anova_lm(results, typ=2)\n",
    "aov_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
